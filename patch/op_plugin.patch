diff --git a/mindspore/ccsrc/include/common/utils/utils.h b/mindspore/ccsrc/include/common/utils/utils.h
index 153e7b3fd19..64d489fa192 100644
--- a/mindspore/ccsrc/include/common/utils/utils.h
+++ b/mindspore/ccsrc/include/common/utils/utils.h
@@ -74,6 +74,7 @@ constexpr auto kFlagGeKernel = "ge_kernel";
 
 // custom operator func type
 constexpr auto kCustomTypeAOT = "aot";
+constexpr auto kCustomTypeOPPlugin = "op_plugin";
 constexpr auto kCustomTypeJULIA = "julia";
 constexpr auto kCustomTypePyfunc = "pyfunc";
 constexpr auto kCustomTypeTbe = "tbe";
diff --git a/mindspore/ccsrc/plugin/device/cpu/hal/device/kernel_select_cpu.cc b/mindspore/ccsrc/plugin/device/cpu/hal/device/kernel_select_cpu.cc
index 96c47732be7..99d48a673d3 100644
--- a/mindspore/ccsrc/plugin/device/cpu/hal/device/kernel_select_cpu.cc
+++ b/mindspore/ccsrc/plugin/device/cpu/hal/device/kernel_select_cpu.cc
@@ -36,6 +36,7 @@
 #include "plugin/device/cpu/kernel/cpu_kernel.h"
 #include "plugin/device/cpu/kernel/custom/custom_aot_cpu_kernel.h"
 #include "plugin/device/cpu/kernel/custom/custom_julia_cpu_kernel.h"
+#include "plugin/device/cpu/kernel/custom/custom_op_plugin_kernel.h"
 #include "plugin/device/cpu/kernel/pyfunc/py_func_cpu_kernel.h"
 #include "common/ms_factory.h"
 #include "utils/trace_base.h"
@@ -679,6 +680,9 @@ void SetCustomOpKernelInfo(const std::string &custom_op_type, const std::string
   } else if (custom_op_type == kCustomTypeJULIA) {
     kernel::Factory<kernel::NativeCpuKernelMod>::Instance().Register(
       op_name, []() { return std::make_shared<kernel::CustomJULIACpuKernelMod>(); });
+  } else if (custom_op_type == kCustomTypeOPPlugin) {
+    kernel::Factory<kernel::NativeCpuKernelMod>::Instance().Register(
+      op_name, []() { return std::make_shared<kernel::CustomOpPluginCpuKernelMod>(); });
   } else {
     MS_LOG(EXCEPTION) << "Unsupported func type for Custom operator on CPU, it should be "
                       << "'hybrid', 'akg', 'pyfunc' or 'aot' or 'julia', "
@@ -734,6 +738,12 @@ std::pair<std::string, ExceptionType> SetKernelInfoWithMsg(const CNodePtr &kerne
   std::vector<kernel::KernelAttr> object_selected_kernel_attrs;
   const auto &kernel_attrs = kernel::NativeCpuKernelMod::GetCpuSupportedList(op_name);
   if (kernel_attrs.empty()) {
+    if (common::EnvHelper::GetInstance()->GetEnv("MS_OP_PLUGIN_PATH") != nullptr) {
+      // if env var MS_OP_PLUGIN_PATH is set, then use custom op plugin to load op
+      SetCustomOpKernelInfo(kCustomTypeOPPlugin, op_name);
+      UpdateCustomKernelBuildInfo(kernel_node, false);
+      return {};
+    }
     return KernelNotSupportWarning(kernel_node, false);
   } else if (kernel_attrs[0].GetSkipCheck()) {
     object_selected_kernel_attrs = kernel_attrs;
diff --git a/mindspore/ccsrc/plugin/device/cpu/kernel/custom/custom_kernel_input_info.h b/mindspore/ccsrc/plugin/device/cpu/kernel/custom/custom_kernel_input_info.h
new file mode 100644
index 00000000000..82366c753e6
--- /dev/null
+++ b/mindspore/ccsrc/plugin/device/cpu/kernel/custom/custom_kernel_input_info.h
@@ -0,0 +1,99 @@
+/**
+ * Copyright 2022 Huawei Technologies Co., Ltd
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef MINDSPORE_CCSRC_PLUGIN_DEVICE_CPU_KERNEL_CUSTOM_CUSTOM_KERNEL_INPUT_INFO_H_
+#define MINDSPORE_CCSRC_PLUGIN_DEVICE_CPU_KERNEL_CUSTOM_CUSTOM_KERNEL_INPUT_INFO_H_
+
+#include <string>
+#include <vector>
+#include "common/kernel_tensor.h"
+
+namespace mindspore {
+class CustomKernelData {
+ public:
+  CustomKernelData() = default;
+  virtual ~CustomKernelData() = default;
+};
+
+class KernelInputInfo {
+ public:
+  KernelInputInfo() = default;
+  virtual ~KernelInputInfo() = default;
+
+  template <typename T>
+  inline T GetKernelInput(size_t) const {
+    return T();
+  }
+
+  void SetWorkSpace(const std::vector<size_t> &workspace) { workspace_ = workspace; }
+  const std::vector<size_t> &WorkSpace() const { return workspace_; }
+
+  void SetKernelData(CustomKernelData *kernel_data) { kernel_data_ = kernel_data; }
+  const CustomKernelData *KernelData() const { return kernel_data_; }
+
+  void DestructKernelData() {
+    delete kernel_data_;
+    kernel_data_ = nullptr;
+  }
+  virtual size_t GetInputSize() = 0;
+
+ private:
+  virtual bool GetBoolInput(size_t idx) = 0;
+  virtual int64_t GetIntInput(size_t idx) = 0;
+  virtual float GetFloatInput(size_t idx) = 0;
+  virtual std::string GetStrInput(size_t idx) = 0;
+
+  virtual std::vector<int64_t> GetIntVecInput(size_t idx) = 0;
+  virtual std::vector<float> GetFloatVecInput(size_t idx) = 0;
+  virtual std::vector<std::vector<int64_t>> GetInt2DVecInput(size_t idx) = 0;
+  virtual std::vector<std::vector<float>> GetFloat2DVecInput(size_t idx) = 0;
+  std::vector<size_t> workspace_;
+
+  CustomKernelData *kernel_data_{nullptr};
+};
+
+class KernelInputInfoImpl : public KernelInputInfo {
+ public:
+  KernelInputInfoImpl() = default;
+  virtual ~KernelInputInfoImpl() = default;
+  void SetKernelInput(const std::vector<kernel::KernelTensor *> &inputs) { inputs_ = inputs; }
+  size_t GetInputSize() { return inputs_.size(); }
+
+ private:
+  bool GetBoolInput(size_t idx) { return inputs_[idx]->GetValueWithCheck<bool>(); }
+
+  int64_t GetIntInput(size_t idx) { return inputs_[idx]->GetValueWithCheck<int64_t>(); }
+
+  float GetFloatInput(size_t idx) { return inputs_[idx]->GetValueWithCheck<float>(); }
+
+  std::string GetStrInput(size_t idx) { return inputs_[idx]->GetValueWithCheck<std::string>(); }
+
+  std::vector<int64_t> GetIntVecInput(size_t idx) { return inputs_[idx]->GetValueWithCheck<std::vector<int64_t>>(); }
+
+  std::vector<float> GetFloatVecInput(size_t idx) { return inputs_[idx]->GetValueWithCheck<std::vector<float>>(); }
+
+  std::vector<std::vector<int64_t>> GetInt2DVecInput(size_t idx) {
+    return inputs_[idx]->GetValueWithCheck<std::vector<std::vector<int64_t>>>();
+  }
+
+  std::vector<std::vector<float>> GetFloat2DVecInput(size_t idx) {
+    return inputs_[idx]->GetValueWithCheck<std::vector<std::vector<float>>>();
+  }
+
+  std::vector<kernel::KernelTensor *> inputs_;
+};
+}  // namespace mindspore
+#endif  // MINDSPORE_CCSRC_PLUGIN_DEVICE_CPU_KERNEL_CUSTOM_CUSTOM_KERNEL_INPUT_INFO_H_
diff --git a/mindspore/ccsrc/plugin/device/cpu/kernel/custom/custom_op_plugin_kernel.cc b/mindspore/ccsrc/plugin/device/cpu/kernel/custom/custom_op_plugin_kernel.cc
new file mode 100644
index 00000000000..9684ccf6a43
--- /dev/null
+++ b/mindspore/ccsrc/plugin/device/cpu/kernel/custom/custom_op_plugin_kernel.cc
@@ -0,0 +1,180 @@
+/**
+ * Copyright 2021-2024 Huawei Technologies Co., Ltd
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include "plugin/device/cpu/kernel/custom/custom_op_plugin_kernel.h"
+
+#if !defined(_WIN32) && !defined(_WIN64)
+#include <dlfcn.h>
+#endif
+
+#include <vector>
+#include <map>
+#include <string>
+#include <algorithm>
+#include <functional>
+#include "abstract/utils.h"
+#include "plugin/device/cpu/hal/device/cpu_common.h"
+#include "utils/file_utils.h"
+#include "utils/ms_utils.h"
+
+namespace mindspore {
+namespace kernel {
+CustomOpPluginCpuKernelMod::~CustomOpPluginCpuKernelMod() {
+#if !defined(_WIN32) && !defined(_WIN64)
+  if (handle_ != nullptr) {
+    dlclose(handle_);
+  }
+
+#endif
+}
+
+void CustomOpPluginCpuKernelMod::SetKernelPath() {
+  const char *op_plugin_path = common::EnvHelper::GetInstance()->GetEnv("MS_OP_PLUGIN_PATH");
+
+  if (op_plugin_path == nullptr) {
+    MS_LOG(EXCEPTION) << "Try to select kernel: " << primitive_->name() << ", but MS_OP_PLUGIN_PATH is not set";
+  }
+
+  auto real_path = FileUtils::GetRealPath(op_plugin_path);
+  file_path_ = real_path.value();
+  func_name_ = primitive_->name();
+}
+
+bool CustomOpPluginCpuKernelMod::Init(const std::vector<KernelTensor *> &inputs,
+                                      const std::vector<KernelTensor *> &outputs) {
+  kernel_name_ = primitive_->name();
+  SetKernelPath();
+
+  for (size_t i = 0; i < inputs.size(); i++) {
+    auto in_shape = inputs[i]->GetShapeVector();
+    auto dtype = inputs[i]->dtype_id();
+    (void)shape_list_.emplace_back(in_shape);
+    ndims_.push_back(SizeToInt(in_shape.size()));
+    (void)type_list_.emplace_back(TypeIdToString(dtype, true));
+  }
+
+  for (size_t i = 0; i < outputs.size(); i++) {
+    auto out_shape = outputs[i]->GetShapeVector();
+    auto dtype = outputs[i]->dtype_id();
+    (void)shape_list_.emplace_back(out_shape);
+    ndims_.push_back(SizeToInt(out_shape.size()));
+    (void)type_list_.emplace_back(TypeIdToString(dtype, true));
+  }
+
+  (void)std::transform(std::begin(shape_list_), std::end(shape_list_), std::back_inserter(shapes_),
+                       [](auto &v) { return &v[0]; });
+  (void)std::transform(std::begin(type_list_), std::end(type_list_), std::back_inserter(type_pointer_list_),
+                       [](auto &str) { return str.c_str(); });
+
+  return true;
+}
+
+bool CustomOpPluginCpuKernelMod::Launch(const std::vector<KernelTensor *> &inputs,
+                                        const std::vector<KernelTensor *> &workspace,
+                                        const std::vector<KernelTensor *> &outputs) {
+  std::vector<void *> params;
+  kernel_info_.SetKernelInput(inputs);
+
+  for (size_t i = 0; i < inputs.size(); i++) {
+    params.push_back(static_cast<void *>(inputs[i]->device_ptr()));
+  }
+  for (size_t i = 0; i < outputs.size(); i++) {
+    params.push_back(static_cast<void *>(outputs[i]->device_ptr()));
+  }
+
+  for (size_t i = 0; i < workspace.size(); i++) {
+    params.push_back(static_cast<void *>(workspace[i]->device_ptr()));
+  }
+
+#if !defined(_WIN32) && !defined(_WIN64)
+
+  if (!handle_) {
+    handle_ = dlopen(file_path_.c_str(), RTLD_LAZY | RTLD_LOCAL);
+    if (!handle_) {
+      MS_LOG(ERROR) << "For '" << kernel_name_ << "' on CPU, dlopen file '" << file_path_
+                    << "' should be successful, but error occurs! Error message is: " << dlerror();
+      return false;
+    }
+  }
+
+  if (!aot_func_) {
+    aot_func_ =
+      reinterpret_cast<std::add_pointer<int(int, void **, int *, int64_t **, const char **, void *, void *)>::type>(
+        dlsym(handle_, func_name_.c_str()));
+    if (auto error_info = dlerror(); error_info != nullptr) {
+      MS_LOG(EXCEPTION) << "For '" << kernel_name_ << "' on CPU, error occurs when fetching function '" << func_name_
+                        << "'. Error info: " << error_info;
+    }
+  }
+
+  int nparam = SizeToInt(params.size());
+  int ret = 0;
+  try {
+    if (nparam == 0) {
+      ret = aot_func_(0, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr);
+    } else {
+      ret = aot_func_(nparam, &params[0], &ndims_[0], &shapes_[0], &type_pointer_list_[0], nullptr,
+                      reinterpret_cast<void *>(&kernel_info_));
+    }
+  } catch (const std::exception &e) {
+    MS_LOG(EXCEPTION) << "For '" << kernel_name_ << "' on CPU, operator failed when executing user defined file "
+                      << file_path_ << "! "
+                      << "Error message is " << e.what();
+  }
+
+  if (ret != 0) {
+    MS_LOG(EXCEPTION) << "Return value from CPU AOT kernel(" << file_path_ << ")'s function(" << func_name_ << ") is "
+                      << ret << ". "
+                      << "Any return value not equal to 0 will be treated as user defined error code and we will "
+                         "terminate execution. If termination is not your purpose, please set return value to 0.";
+  }
+
+#else
+  MS_LOG(EXCEPTION) << "Custom AOT Operator doesn't support Windows currently";
+#endif
+
+  return true;
+}
+
+int CustomOpPluginCpuKernelMod::Resize(const std::vector<KernelTensor *> &inputs,
+                                       const std::vector<KernelTensor *> &outputs) {
+  int ret = KernelMod::Resize(inputs, outputs);
+  if (ret != 0) {
+    return ret;
+  }
+  // kernel_info_.SetKernelInput(inputs);
+  shapes_.clear();
+  shape_list_.clear();
+  ndims_.clear();
+
+  for (size_t i = 0; i < inputs.size(); i++) {
+    auto in_shape = inputs[i]->GetShapeVector();
+    (void)shape_list_.emplace_back(in_shape);
+    ndims_.push_back(SizeToInt(in_shape.size()));
+  }
+
+  for (size_t i = 0; i < outputs.size(); i++) {
+    auto out_shape = outputs[i]->GetShapeVector();
+    (void)shape_list_.emplace_back(out_shape);
+    ndims_.push_back(SizeToInt(out_shape.size()));
+  }
+
+  (void)std::transform(std::begin(shape_list_), std::end(shape_list_), std::back_inserter(shapes_),
+                       [](auto &v) { return &v[0]; });
+
+  return static_cast<int>(KRET_OK);
+}
+}  // namespace kernel
+}  // namespace mindspore
diff --git a/mindspore/ccsrc/plugin/device/cpu/kernel/custom/custom_op_plugin_kernel.h b/mindspore/ccsrc/plugin/device/cpu/kernel/custom/custom_op_plugin_kernel.h
new file mode 100644
index 00000000000..73890c44285
--- /dev/null
+++ b/mindspore/ccsrc/plugin/device/cpu/kernel/custom/custom_op_plugin_kernel.h
@@ -0,0 +1,62 @@
+/**
+ * Copyright 2021-2022 Huawei Technologies Co., Ltd
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef MINDSPORE_CCSRC_PLUGIN_DEVICE_CPU_KERNEL_CUSTOM_CUSTOM_OP_PLUGIN_CPU_KERNEL_H_
+#define MINDSPORE_CCSRC_PLUGIN_DEVICE_CPU_KERNEL_CUSTOM_CUSTOM_OP_PLUGIN_CPU_KERNEL_H_
+
+#include <vector>
+#include <string>
+#include <map>
+#include "plugin/device/cpu/kernel/custom/custom_kernel_input_info.h"
+#include "plugin/device/cpu/kernel/cpu_kernel.h"
+
+namespace mindspore {
+namespace kernel {
+
+class OPS_KERNEL_COMMON_API CustomOpPluginCpuKernelMod : public NativeCpuKernelMod {
+ public:
+  CustomOpPluginCpuKernelMod() : handle_(nullptr), resize_func_(nullptr), aot_func_(nullptr) {}
+  ~CustomOpPluginCpuKernelMod();
+
+  bool Init(const std::vector<KernelTensor *> &inputs, const std::vector<KernelTensor *> &outputs) override;
+  bool Launch(const std::vector<KernelTensor *> &inputs, const std::vector<KernelTensor *> &workspace,
+              const std::vector<KernelTensor *> &outputs) override;
+  int Resize(const std::vector<KernelTensor *> &inputs, const std::vector<KernelTensor *> &outputs) override;
+
+ protected:
+  std::vector<std::vector<int64_t>> shape_list_;
+  std::vector<int> ndims_;
+  std::vector<std::string> type_list_;
+
+  std::vector<int64_t *> shapes_;
+  std::vector<const char *> type_pointer_list_;
+
+  std::string file_path_;
+  std::string func_name_;
+  void *handle_{nullptr};
+  // int (*init_func_)(int *, int64_t **, const char **, KernelInputInfo *);
+  int (*resize_func_)(int *, int64_t **, const char **, KernelInputInfo *);
+  int (*aot_func_)(int, void **, int *, int64_t **, const char **, void *, void *);
+
+  KernelInputInfoImpl kernel_info_;
+
+ private:
+  void SetKernelPath();
+};
+}  // namespace kernel
+}  // namespace mindspore
+
+#endif  // MINDSPORE_CCSRC_PLUGIN_DEVICE_CPU_KERNEL_CUSTOM_CUSTOM_OP_PLUGIN_CPU_KERNEL_H_
diff --git a/mindspore/ccsrc/pyboost/pyboost_utils.cc b/mindspore/ccsrc/pyboost/pyboost_utils.cc
index c8380c81854..949e113965d 100644
--- a/mindspore/ccsrc/pyboost/pyboost_utils.cc
+++ b/mindspore/ccsrc/pyboost/pyboost_utils.cc
@@ -33,6 +33,7 @@
 #include "mindspore/ops/op_def/array_ops.h"
 #include "include/common/runtime_conf/runtime_conf.h"
 #include "mindspore/ops/op_def/auto_generate/gen_ops_primitive_c.h"
+#include "plugin/device/cpu/kernel/custom/custom_op_plugin_kernel.h"
 
 namespace mindspore {
 namespace kernel {
@@ -173,7 +174,12 @@ kernel::KernelModPtr PyBoostUtils::CreateKernelMod(const PrimitivePtr &prim, con
   if (kernel_mod == nullptr) {
     kernel_mod = device_context->GetKernelExecutor(false)->CreateKernelMod(op_name);
     if (kernel_mod == nullptr) {
-      MS_LOG(EXCEPTION) << "Create kernelmod for op " << op_name << " failed";
+      if (common::EnvHelper::GetInstance()->GetEnv("MS_OP_PLUGIN_PATH") != nullptr) {
+        // if env var MS_OP_PLUGIN_PATH is set, then use custom op plugin to load op
+        kernel_mod = std::make_shared<kernel::CustomOpPluginCpuKernelMod>();
+      } else {
+        MS_LOG(EXCEPTION) << "Create kernelmod for op " << op_name << " failed";
+      }
     }
     if (!kernel_mod->Init(prim, inputs, outputs)) {
       MS_LOG(EXCEPTION) << "KernelMod Init Failed: " << op_name;
@@ -586,8 +592,14 @@ std::pair<bool, KernelAttr> PyBoostUtils::SelectKernel(const std::vector<Abstrac
                                                        const DeviceContext *device_context,
                                                        const std::string &op_name) {
   // only support CPU
-  const auto &kernel_mod = device_context->GetKernelExecutor(false)->CreateKernelMod(op_name);
+  auto kernel_mod = device_context->GetKernelExecutor(false)->CreateKernelMod(op_name);
   if (kernel_mod == nullptr) {
+    if (common::EnvHelper::GetInstance()->GetEnv("MS_OP_PLUGIN_PATH") != nullptr) {
+      // if env var MS_OP_PLUGIN_PATH is set, then use custom op plugin to load op
+      MS_LOG(WARNING) << "The kernel " << op_name << " unregistered, use custom op plugin";
+      kernel_mod = std::make_shared<kernel::CustomOpPluginCpuKernelMod>();
+      return {true, KernelAttr()};
+    }
     MS_LOG(EXCEPTION) << "The kernel " << op_name << " unregistered.";
   }
   return GetKernelAttr(op_name, kernel_mod, GetInputTypeFromAbstractBase(inputs_abs),
diff --git a/mindspore/ops/op_def/yaml/acos_ext_op.yaml b/mindspore/ops/op_def/yaml/acos_ext_op.yaml
index d786e94a0db..f5cd6ecdcb5 100644
--- a/mindspore/ops/op_def/yaml/acos_ext_op.yaml
+++ b/mindspore/ops/op_def/yaml/acos_ext_op.yaml
@@ -10,5 +10,4 @@ acos_ext:
         name: AcosExt
     dispatch:
         enable: True
-        CPU: None
         GPU: None
diff --git a/mindspore/ops/op_def/yaml/atan_ext_op.yaml b/mindspore/ops/op_def/yaml/atan_ext_op.yaml
index 7a16fc8d666..c9e48768506 100644
--- a/mindspore/ops/op_def/yaml/atan_ext_op.yaml
+++ b/mindspore/ops/op_def/yaml/atan_ext_op.yaml
@@ -10,5 +10,4 @@ atan_ext:
         name: AtanExt
     dispatch:
         enable: True
-        CPU: None
         GPU: None
diff --git a/mindspore/ops/op_def/yaml/stack_ext_op.yaml b/mindspore/ops/op_def/yaml/stack_ext_op.yaml
index 297194e923f..fb5a6ce0c95 100644
--- a/mindspore/ops/op_def/yaml/stack_ext_op.yaml
+++ b/mindspore/ops/op_def/yaml/stack_ext_op.yaml
@@ -13,5 +13,4 @@ stack_ext:
             dtype: tensor
     dispatch:
         enable: True
-        CPU: None
         GPU: None
diff --git a/mindspore/ops/op_def/yaml/zeros_like_ext_op.yaml b/mindspore/ops/op_def/yaml/zeros_like_ext_op.yaml
index f300c3c8677..f8fe63d5f6a 100644
--- a/mindspore/ops/op_def/yaml/zeros_like_ext_op.yaml
+++ b/mindspore/ops/op_def/yaml/zeros_like_ext_op.yaml
@@ -15,5 +15,4 @@ zeros_like_ext:
     dispatch:
         enable: True
         Ascend: ZerosLikeExtAscend
-        CPU: None
         GPU: None
